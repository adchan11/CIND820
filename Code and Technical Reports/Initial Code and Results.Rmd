---
title: "Applications of Supervised Machine Learning Methods for the Prediction of Myocardial Infarction Complications in Hospital Patients"
author: 
output:
  html_document: default
  word_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
---
<center> <h1> Initial Results and Code </h1> </center>
<center>  <h3> [Author: Adrian Chan] </h2> </center>
<center>  <h3> [Supervisor: Dr. Sedef Akinli Kocak] </h2> </center>
<center> <h3> [CIND 820: DA0 & 501051583] </h2> </center>
---
******
\newpage
## Data Cleaning

Data is loaded and the class variable is created. Features are selected based on the research questions.
```{r}
#Load libraries
library(dplyr)
library(tidyr)

#Load data

df_raw <- read.table("https://leicester.figshare.com/ndownloader/files/23581310", sep = ",", header = TRUE, stringsAsFactors = FALSE)

#Create new class variable

df_comp <- df_raw[113:124]

df_comp$LET_IS_binary <- ifelse(df_comp$LET_IS == 0, 0, 1)

df_comp <- df_comp %>%
  select(-LET_IS)

df_comp$any_complication <- rowSums(df_comp)

df_comp$any_complication <- ifelse(df_comp$any_complication == 0, 0, 1)

df_raw$any_complication <- df_comp$any_complication

#Select only relevant features based on scope of research questions
medical_history_var <- c(4:8, 11:12, 13:34)
symptoms_var <-c(9:10, 35:43, 49, 83:91)
drugs_var <- c(75:82, 96:112)

df <- df_raw %>% 
  dplyr:::select(c(2:3, all_of(medical_history_var), all_of(symptoms_var), all_of(drugs_var), "any_complication"))
```

Outliers are removed and transformations are done on select input features to ensure consistency and data quality.
```{r}
#Transform Blood Pressure Variables

#Filter if any of the blood pressure variables equal 0
df_blood_invalid <- df %>% 
  filter(if_any(c(S_AD_KBRIG, D_AD_KBRIG, S_AD_ORIT, D_AD_ORIT), ~ . == 0)) 
head(df_blood_invalid, 10)

#Remove rows where any of the blood pressure variables equal 0

df <- df %>% 
  mutate(has_zero = if_any(c(S_AD_KBRIG, D_AD_KBRIG, S_AD_ORIT, D_AD_ORIT), ~ . == 0)) %>%
  mutate(has_zero = ifelse(is.na(has_zero), FALSE, has_zero)) %>%
  filter(has_zero == FALSE) %>%
  select(-has_zero)

#Transform new systolic and diastolic variables 

df$Systolic <- ifelse(is.na(df$S_AD_KBRIG) | is.na(df$S_AD_ORIT), coalesce(df$S_AD_KBRIG, df$S_AD_ORIT), 
                      (df$S_AD_KBRIG + df$S_AD_ORIT) / 2)

length(which(is.na(df$Systolic)))

df$Diastolic <- ifelse(is.na(df$D_AD_KBRIG) | is.na(df$D_AD_ORIT), coalesce(df$D_AD_KBRIG, df$D_AD_ORIT), 
                      (df$D_AD_KBRIG + df$D_AD_ORIT) / 2)

length(which(is.na(df$Diastolic)))

#Remove pre-existing blood pressure variables and just keep Systolic blood pressure

df <- df %>%
  select(-c('S_AD_KBRIG', 'S_AD_ORIT', 'D_AD_KBRIG', 'D_AD_ORIT', 'Diastolic')) %>%
  relocate(Systolic, .before = K_BLOOD)
```

******
\newpage
## Exploratory Analysis

Distribution of Attributes
-variables with too much missing data are removed
```{r}
#Create histograms 

misfun <- function(z) {paste(
  paste(c('n', 'm'), table(factor(as.double(is.na(z)), levels=0:1)), sep=':'), 
  collapse=' ')}

num1 <- 1
num2 <- 20
for (i in 1:(ceiling(length(df)/20))) {

if (i == 4) {
  num2 <- 75
}
png(paste0('hist', i, '.png'), 1000, 1000)
par(mfrow=c(5,4))

df_new <- df[num1:num2]
lapply(names(df_new), function(x) hist(df_new[[x]], main=x, xlab=misfun(df_new[x]),  cex.lab=1.5, cex.axis=1.5, cex.main = 1.5, cex.sub=1.5,
                                   breaks = 'FD'))
dev.off()
num1 <- num2 + 1
num2 <- 20 * (1 + i)
}

#Drop variables with too much missing data

df <- df %>%
  select(-c("KFK_BLOOD", "IBS_NASL"))
```

Boxplots of Numeric Attributes
-non-significant features are removed
```{r fig.align="center", echo = FALSE,fig.height = 10, fig.width = 10}
#Box plots using t-test
library(rstatix)
library(ggpubr)

#Do not include KFK_BLOOD (variable 89) as it only has 4 records and the rest are missing values
numeric_cols = c("AGE", "Systolic", "K_BLOOD", "NA_BLOOD", "ALT_BLOOD", "AST_BLOOD", "L_BLOOD", "ROE")

df_numeric <- df %>% 
  select(any_of(c(numeric_cols, "any_complication")))

df_long <- df_numeric %>%
  pivot_longer(-`any_complication`, names_to = "variables", values_to = "value") %>%
  as.data.frame()

#Remove NA values
df_long <- df_long[!is.na(df_long$value),]

#Generate boxplots and include t-test significance results
df_stats <- df_long %>%
  group_by(variables) %>%
  t_test(value ~ any_complication) %>%
  # adjust_pvalue(method = "BH") %>%
  add_significance()

df_boxplots <- ggboxplot(
  df_long, x = "any_complication", y = "value",
  fill = "any_complication", palette = "npg", legend = "none", order = NULL,
  ggtheme = theme_pubr(border = TRUE)
  ) +
  facet_wrap(~variables ,scales = "free_y") 

df_stats <- df_stats %>% 
  add_xy_position(x = "any_complication", scales = 'free', step.increase = 1) 


df_boxplots +stat_pvalue_manual(df_stats, label = "p.signif") + 
 font("title", size = 14,  face = "bold")+
 font("xy.text", size = 12, face = "bold")

#Remove non significant variables
# 
# df <- df %>%
#   select(-c('ALT_BLOOD', 'AST_BLOOD'))
```
Correlation Plot for Numeric Variables
```{r}
#Correlation plot for numeric variables
#Point-Biserial coefficient with t-test for significance
library(corrplot)

#Select only numeric variables
df_corr_numeric <- df_numeric 

#Generate significance values at confidence level = 0.95
testRes <- cor.mtest(df_corr_numeric, conf.level = 0.95) 

#Generate correlation plot with statistical significance
corrplot(cor(df_corr_numeric, use = 'pairwise.complete.obs'), p.mat = testRes$p, method = "color", diag = FALSE, type = 'upper',
         sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9, 
         insig = 'label_sig', pch.col = 'purple')


# Reference: http://web.pdx.edu/~newsomj/pa551/lectur15.htm
```

Correlation Plot for Binary Variables
```{r fig.align="center", echo = FALSE,fig.height = 10, fig.width = 10}
#Correlation plot for binary variables
#Phi Coefficient correlation
library(corrplot)

#Select binary attributes only
binary_cols <- c(3, 8, 10, 13:34, 39:44, 49:74, 76:83, 85, 96:99, 106:112)
df_corr_binary_raw <- df_raw[binary_cols]
df_corr_binary <- df %>% 
  select(any_of(c(colnames(df_corr_binary_raw), "any_complication")))

#Split into two datasets so that the correlation plot is easier to read 

df_corr_binary_1 <- df_corr_binary[c(1:24, 51)]
df_corr_binary_2 <- df_corr_binary[25:51]

#Function to generate a matrix of chi-square test values
chisqmatrix <- function(x) {
  names <- colnames(x);  num = length(names)
  m <- matrix(nrow=num,ncol=num,dimnames=list(names,names))
  for (i in 1:(num-1)) {
    for (j in (i+1):num) {
      m[i,j] <- tryCatch(
        {chisq.test(x[,i],x[,j],simulate.p.value = TRUE)$p.value
        },
        error=function(x){
          return(NA)
        }
      )
    }
  }
  return (m)
}

#Generate significance values
testRes_1 <- chisqmatrix(df_corr_binary_1)
testRes_2 <- chisqmatrix(df_corr_binary_2)

#Generate correlation plot with statistical significance
corrplot(cor(df_corr_binary_1, use = "pairwise.complete.obs"), p.mat = testRes_1, method = "color", diag = FALSE, type = 'upper',
         sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.8, 
         insig = 'label_sig', pch.col = 'purple', na.label = "NA", number.cex = 0.5, tl.cex = 0.8)

corrplot(cor(df_corr_binary_2, use = "pairwise.complete.obs"), p.mat = testRes_2, method = "color", diag = FALSE, type = 'upper',
         sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.8, 
         insig = 'label_sig', pch.col = 'purple', na.label = "NA", number.cex = 0.5, tl.cex = 0.8)

# References:
# https://stat.ethz.ch/pipermail/r-help/2015-April/428127.html
# http://web.pdx.edu/~newsomj/pa551/lectur15.htm
# http://www.ce.memphis.edu/7012/L17_CategoricalVariableAssociation.pdf
```

Correlation Plot for Categorical Variables
```{r fig.align="center", echo = FALSE,fig.height = 10, fig.width = 10}
#Correlation plot for categorical variables
#CramÃ©r's V
library(corrplot)

#Select ordinal variables only
ordinal_cols <- c(4, 5, 6, 7, 9, 11, 12, 45:48, 92:95, 100:105)

df_corr_ordinal_raw <- df_raw[ordinal_cols]
df_corr_ordinal <- df %>% 
  select(any_of(c(colnames(df_corr_ordinal_raw), "any_complication")))

#Create matrix of chi-square test values
mat <- chisqmatrix(df_corr_ordinal)

#Generate correlation plot with chi-square test significance 
corrplot::corrplot(DescTools::PairApply(df_corr_ordinal, DescTools::CramerV), p.mat = mat, method = 'color', diag = FALSE, type = 'upper',
         sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9, 
         insig = 'label_sig', pch.col = 'purple', na.label = "NA", number.cex = 0.5, tl.cex = 0.8)

# Reference:
# https://stackoverflow.com/questions/32732582/chi-square-p-value-matrix-in-r
```
\newpage
## Dimensionality Reduction

Feature Selection
```{r}
not_sig_cols <- c('ALT_BLOOD', 'AST_BLOOD', "SIM_GIPERT", "nr_11", 'nr_02', 'nr_03', 'nr_04', 'nr_07', 'nr_08', 'np_01', 'np_04', 'np_05', 'np_07', 'np_08', 'np_09', 'np_10', 'endocr_02', 'endocr_03', 'zab_leg_02', 'zab_leg_03', 'zab_leg_04', 'SVT_POST', 'GT_POST', 'fibr_ter_01', 'fibr_ter_03', 'fibr_ter_05', 'fibr_ter_06', 'fibr_ter_07', 'fibr_ter_08', 'GIPER_NA', 'LID_KB', 'GEPAR_S_n', 'TRENT_S_n', 'NOT_NA_1_n', 'NOT_NA_2_n', 'NOT_NA_3_n')

df <- df %>% select(-all_of(not_sig_cols))
```

Feature Importance
```{r}
#Feature Importance using the LVQ Method
set.seed(1)
# load the library
library(mlbench)
library(caret)

#Imputation of missing values using the median
for(i in 1:ncol(df)) {                                   # Replace NA in all columns
  df[ , i][is.na(df[ , i])] <- median(df[ , i], na.rm = TRUE)
}

df_lvq <- df

#Convert class variable to factor
df_lvq$any_complication <- as.character(df_lvq$any_complication)
df_lvq$any_complication <- as.factor(df_lvq$any_complication)

# prepare training scheme 
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
# train the model
model <- train(any_complication ~., data = df_lvq, method = "lvq", preProcess = "scale", trControl = control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)

#Obtain list of top 20 important variables
library(tibble)
imp_cols <- importance$importance 
imp_cols <- tibble::rownames_to_column(imp_cols, "Variables")
imp_cols_string <- imp_cols[1:20, 'Variables']

#Compare feature importance with correlation analysis 
Reduce(intersect, list(imp_cols_string, colnames(df)))

#Reference
# https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/

```

\newpage
## Data Pre Processing

```{r}
#Data Scaling

#Custom function to implement min max scaling
minMax <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

#Apply minMax function to all numeric attributes
df <- df %>% 
  mutate(across(any_of(numeric_cols), minMax))

#Data Normalization - automatically select the best method
# library(bestNormalize)
# 
# df2 <- df2 %>% 
#   mutate(across(all_of(numeric_cols), ))

#Reference:
# https://www.geeksforgeeks.org/how-to-normalize-and-standardize-data-in-r/
# https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html

```

******
\newpage
## Train/Test Split
```{r}
#Train/test split
index <- sort(sample(nrow(df), nrow(df)*.7))

#Convert class variable to factor
df$any_complication <- as.character(df$any_complication)
df$any_complication <- as.factor(df$any_complication)

train <- df[index,]
test <- df[-index,]

#Apply SMOTE to fix imbalanced class distribution
library(DMwR)
set.seed(1)
train_smote <- DMwR::SMOTE(any_complication ~ ., data = train, perc.over = 154)

table(train_smote$any_complication)


# medical_history_cols <- c(4:8, 11:12, 13:34)
# symptoms_cols <- c(9:10, 35:43, 49, 83:91)
# drugs_cols <- c(76:82, 96:112)
# missing_values_cols <- c('KFK_BLOOD', 'S_AD_KBRIG', 'D_AD_KBRIG', 'IBS_NASL')
# new_train <- train[c(medical_history_cols, symptoms_cols, drugs_cols, 125)] %>%
#   select(-all_of(missing_values_cols))
# 
# new_test <- test %>% 
#   select(all_of(colnames(new_train)))
#   
# lapply(new_train, function(x) length(which(is.na(x))))

#Reference
# https://www.statology.org/smote-in-r/
```

```{r}
#Logistic Regression
set.seed(1)
# sig_cols <- c('LID_S_n', 'NITR_S', 'NOT_NA_KB', 'NA_BLOOD', 'O_L_POST', 'zab_leg_02', 'zab_leg_01', 'endocr_01')
# train_final <- new_train %>% select(all_of(c(sig_cols, 'any_complication')))
# test_final <- new_test %>% select(all_of(c(sig_cols, 'any_complication')))

system.time(full_LR_model <- glm(any_complication ~.,family = binomial(link = 'logit'), data = train_smote, na.action = na.omit))
summary(full_LR_model)
# https://datascienceplus.com/perform-logistic-regression-in-r/#:~:text=Logistic%20regression%20is%20a%20method,general%2C%20can%20assume%20different%

library(MASS)
system.time(step_LR_model <- full_LR_model %>% stepAIC(trace = FALSE))
summary(step_LR_model)

#Reference
# http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/#:~:text=The%20stepwise%20logistic%20regression%20can,ref(stepwise%2Dregression)).

#https://www.datacamp.com/community/tutorials/logistic-regression-R
#https://towardsdatascience.com/machine-learning-with-r-logistic-regression-152ec20351db
result <- predict(step_LR_model, newdata = test, type = 'response')

# optCutOff <- optimalCutoff(test$Churn, result)[1] 
preds <- ifelse(result > 0.5, 1,0 )

## Confusion matrix and statistics
caret::confusionMatrix(factor(preds), factor(test$any_complication), mode = "everything", positive = "1")

# https://stackoverflow.com/questions/70905642/error-in-lm-fitx-y-na-nan-inf-in-x-in-r

library(pROC)
roc <- roc(response = test$any_complication, predictor = preds, smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)

#Reference
# https://rviews.rstudio.com/2019/03/01/some-r-packages-for-roc-curves/
# https://github.com/xrobin/pROC/wiki/FAQ---Frequently-asked-questions
```

```{r}
#Naive Bayes
library('e1071')
set.seed(1)
system.time(NB_model <- naiveBayes(as.factor(any_complication) ~ ., data = train_smote))
 # m$tables
 # print(m)

 preds <- predict(NB_model, test)

 # table(y_pred, test$any_complication)
 
caret::confusionMatrix(preds, factor(test$any_complication), mode = "everything", positive = "1")

roc <- roc(response = test$any_complication, predictor = as.numeric(preds), smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)

```

```{r}
#Random Forest
library(randomForest)
set.seed(1)
system.time(RF_model <- randomForest(any_complication ~., data = train_smote, proximity = TRUE, na.action = na.exclude))

preds <- predict(RF_model, test)

caret::confusionMatrix(factor(preds), factor(test$any_complication), mode = "everything", positive = "1")

roc <- roc(response = test$any_complication, predictor = as.numeric(preds), smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)

# https://www.r-bloggers.com/2021/04/random-forest-in-r/
# https://discuss.analyticsvidhya.com/t/what-does-the-warning-the-response-has-five-or-fewer-unique-values-while-building-random-forest-mean/6442/2
```

```{r}
#Neural Network
# library(neuralnet)
# 
# NN_model <- neuralnet(any_complication ~ ., data = train, hidden = c(2,1), linear.output = FALSE, threshold=0.01)
# 
# 
# #Test the resulting output
# test_NN <- test %>% 
#   select(-any_complication)
# 
# nn.results <- compute(NN_model, test_NN)
# results <- data.frame(actual = test$any_complication, prediction = nn.results$net.result)
# 
# roundedresults<-sapply(results,round,digits=0)
# roundedresultsdf=data.frame(roundedresults)
# attach(roundedresultsdf)
# confusionMatrix(table(actual,prediction), positive = "1", mode = 'everything')
# # https://stackoverflow.com/questions/63770095/how-to-print-confusion-matrix-of-neuralnet-predicted-probabilities
# # https://datascienceplus.com/neuralnet-train-and-test-neural-networks-using-r/

library(nnet)
set.seed(1)
system.time(NN_model <- nnet::nnet(any_complication ~ ., data = train_smote, size = 10))
result <- predict(NN_model, newdata = test)
preds <- ifelse(result > 0.5, 1,0 )

confusionMatrix(data = factor(preds), reference = factor(test$any_complication), mode = "everything", positive = "1")

roc <- roc(response = test$any_complication, predictor = as.numeric(preds), smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)

# result <- predict(LR_model, newdata = test, type = 'response')
# 
# # optCutOff <- optimalCutoff(test$Churn, result)[1] 
# preds<- ifelse(result > 0.5, 1,0 )


```